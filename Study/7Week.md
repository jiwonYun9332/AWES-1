
# 7Week - 7ì£¼ì°¨ ì‹¤ìŠµ

![image](https://github.com/jiwonYun9332/AWES-1/blob/ba891951a2e3d48a6534bc1ce66dd04c3a0a98ee/Study/images/118_image.jpg)

### 1. AWS Controller for Kubernetes (ACK)

**AWS Controller for Kubernetesì´ë€?**

```
The idea behind AWS Controllers for Kubernetes (ACK) is to enable Kubernetes users to describe the desired state of AWS resources using the Kubernetes API and configuration language.
```

![image](https://github.com/jiwonYun9332/AWES-1/blob/2f7701c25a215f9476dd2ee8714fea259f59b801/Study/images/119_image.jpg)

AWSì—ëŠ” ìµìˆ™í•˜ì§€ ì•Šì€ ì—”ì§€ë‹ˆì–´ë“¤ì„ ìœ„í•´ Kubernetes ì•ˆì—ì„œ AWS ë¦¬ì†ŒìŠ¤ë“¤ì„ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” Controllerì´ë‹¤.

aws ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ë¥¼ k8sì—ì„œ ì§ì ‘ ì •ì˜í•˜ê³  ì‚¬ìš© í•  ìˆ˜ ìˆë‹¤.

1. kube-apiserver -> ack-s3-controllerì— ìš”ì²­ ì „ë‹¬
ì‚¬ìš©ìëŠ” kubectlë¥¼ ì‚¬ìš©í•˜ì—¬ aws s3 ë²„í‚·ì„ ìƒì„± í•  ìˆ˜ ìˆë‹¤.

2. ack-s3-controllerê°€ IRSA ê¶Œí•œì´ ìˆëŠ” ê²½ìš°, AWS S3 APIë¥¼ í†µí•´ ë²„í‚· ìƒì„±
ì¿ ë²„ë„¤í‹°ìŠ¤ apiëŠ” ack-s3-controllerì— ìš”ì²­ì„ ì „ë‹¬í•˜ê³ , ack-s3-controller(IRSA)ì´ aws s3 apië¥¼ í†µí•´ ë²„í‚·ì„ ìƒì„±í•˜ê²Œ ëœë‹¤.

`ACK ê°€ ì§€ì›í•˜ëŠ” AWS ì„œë¹„ìŠ¤` : (â€˜23.5.29ì¼) í˜„ì¬ **17ê°œ** ì„œë¹„ìŠ¤ëŠ” **GA** General Availability (ì •ì‹ ì¶œì‹œ) ë˜ì—ˆê³ , **10ê°œ**ëŠ” **Preview** (í‰ê°€íŒ) ìƒíƒœì…ë‹ˆë‹¤ - [ë§í¬](https://aws-controllers-k8s.github.io/community/docs/community/services/)

- ****Maintenance Phases**** ê´€ë¦¬ ë‹¨ê³„ : **PREVIEW** (í…ŒìŠ¤íŠ¸ ë‹¨ê³„, ìƒìš© ì„œë¹„ìŠ¤ ë¹„ê¶Œì¥) , **GENERAL AVAILABILITY** (ìƒìš© ì„œë¹„ìŠ¤ ê¶Œì¥), DEPRECATED, NOT SUPPORTED
- **GA** ì„œë¹„ìŠ¤ : ApiGatewayV2, CloudTrail, **DynamoDB**, **EC2**, ECR, EKS, IAM, KMS, **Lambda**, MemoryDB, **RDS**, **S3**, SageMakerâ€¦
- **Preview** ì„œë¹„ìŠ¤ : ACM, ElastiCache, EventBridge, MQ, Route 53, SNS, SQSâ€¦

Permissions : k8s api ì™€ aws api ì˜ 2ê°œì˜ RBAC ì‹œìŠ¤í…œ í™•ì¸, ê° ì„œë¹„ìŠ¤ ì»¨íŠ¸ë¡¤ëŸ¬ íŒŒë“œëŠ” AWS ì„œë¹„ìŠ¤ ê¶Œí•œ í•„ìš” â† IRSA role for ACK Service Controller

**ACK S3 Controller ì„¤ì¹˜ with Helm**

```
# ì„œë¹„ìŠ¤ëª… ë³€ìˆ˜ ì§€ì •
export SERVICE=s3

# helm ì°¨íŠ¸ ë‹¤ìš´ë¡œë“œ
#aws ecr-public get-login-password --region us-east-1 | helm registry login --username AWS --password-stdin public.ecr.aws
export RELEASE_VERSION=$(curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep '"tag_name":' | cut -d'"' -f4 | cut -c 2-)
helm pull oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION
tar xzvf $SERVICE-chart-$RELEASE_VERSION.tgz

# helm chart í™•ì¸
tree ~/$SERVICE-chart

# ACK S3 Controller ì„¤ì¹˜
export ACK_SYSTEM_NAMESPACE=ack-system
export AWS_REGION=ap-northeast-2
helm install --create-namespace -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller --set aws.region="$AWS_REGION" ~/$SERVICE-chart

# ì„¤ì¹˜ í™•ì¸
helm list --namespace $ACK_SYSTEM_NAMESPACE
NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
ack-s3-controller       ack-system      1               2023-06-11 17:24:32.768580184 +0900 KST deployed        s3-chart-1.0.4  1.0.4
kubectl -n ack-system get pods
kubectl get crd | grep $SERVICE
buckets.s3.services.k8s.aws                  2023-06-11T08:24:32Z
kubectl get all -n ack-system
kubectl get-all -n ack-system
kubectl describe sa -n ack-system ack-s3-controller
```

**IRSA ì„¤ì • : ê¶Œì¥ ì •ì±… AmazonS3FullAccess**

```
# Create an iamserviceaccount - AWS IAM role bound to a Kubernetes service account
eksctl create iamserviceaccount \
  --name ack-$SERVICE-controller \
  --namespace ack-system \
  --cluster $CLUSTER_NAME \
  --attach-policy-arn $(aws iam list-policies --query 'Policies[?PolicyName==`AmazonS3FullAccess`].Arn' --output text) \
  --override-existing-serviceaccounts --approve

# í™•ì¸ >> ì›¹ ê´€ë¦¬ ì½˜ì†”ì—ì„œ CloudFormation Stack >> IAM Role í™•ì¸
eksctl get iamserviceaccount --cluster $CLUSTER_NAME

# Inspecting the newly created Kubernetes Service Account, we can see the role we want it to assume in our pod.
kubectl get sa -n ack-system
kubectl describe sa ack-$SERVICE-controller -n ack-system

# Restart ACK service controller deployment using the following commands.
kubectl -n ack-system rollout restart deploy ack-$SERVICE-controller-$SERVICE-chart

# IRSA ì ìš©ìœ¼ë¡œ Env, Volume ì¶”ê°€ í™•ì¸
kubectl describe pod -n ack-system -l k8s-app=$SERVICE-chart
```

**S3 ë²„í‚· ìƒì„±, ì—…ë°ì´íŠ¸, ì‚­ì œ**

```
# [í„°ë¯¸ë„1] ëª¨ë‹ˆí„°ë§
watch -d aws s3 ls

# S3 ë²„í‚· ìƒì„±ì„ ìœ„í•œ ì„¤ì • íŒŒì¼ ìƒì„±
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
export BUCKET_NAME=my-ack-s3-bucket-$AWS_ACCOUNT_ID

read -r -d '' BUCKET_MANIFEST <<EOF
apiVersion: s3.services.k8s.aws/v1alpha1
kind: Bucket
metadata:
  name: $BUCKET_NAME
spec:
  name: $BUCKET_NAME
EOF

echo "${BUCKET_MANIFEST}" > bucket.yaml
cat bucket.yaml | yh

# S3 ë²„í‚· ìƒì„±
aws s3 ls
kubectl create -f bucket.yaml
bucket.s3.services.k8s.aws/my-ack-s3-bucket-<my account id> created

# S3 ë²„í‚· í™•ì¸
aws s3 ls
kubectl get buckets
kubectl describe bucket/$BUCKET_NAME | head -6
Name:         my-ack-s3-bucket-485702506058
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  s3.services.k8s.aws/v1alpha1
Kind:         Bucket

aws s3 ls | grep $BUCKET_NAME
2023-06-11 19:53:29 my-ack-s3-bucket-485702506058

# S3 ë²„í‚· ì—…ë°ì´íŠ¸ : íƒœê·¸ ì •ë³´ ì…ë ¥
read -r -d '' BUCKET_MANIFEST <<EOF
apiVersion: s3.services.k8s.aws/v1alpha1
kind: Bucket
metadata:
  name: $BUCKET_NAME
spec:
  name: $BUCKET_NAME
  tagging:
    tagSet:
    - key: myTagKey
      value: myTagValue
EOF

echo "${BUCKET_MANIFEST}" > bucket.yaml

# S3 ë²„í‚· ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤í–‰ : í•„ìš” ì£¼ì„ ìë™ ì—…ëƒ ë‚´ìš©ì´ë‹ˆ ë¬´ì‹œí•´ë„ë¨!
kubectl apply -f bucket.yaml

# S3 ë²„í‚· ì—…ë°ì´íŠ¸ í™•ì¸ 
kubectl describe bucket/$BUCKET_NAME | grep Spec: -A5
Spec:
  Name:  my-ack-s3-bucket-485702506058
  Tagging:
    Tag Set:
      Key:    myTagKey
      Value:  myTagValue

# S3 ë²„í‚· ì‚­ì œ
kubectl delete -f bucket.yaml

# verify the bucket no longer exists
kubectl get bucket/$BUCKET_NAME
aws s3 ls | grep $BUCKET_NAME
```

**ACK S3 Controller ì‚­ì œ**

```
# helm uninstall
export SERVICE=s3
helm uninstall -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller

# ACK S3 Controller ê´€ë ¨ crd ì‚­ì œ
kubectl delete -f ~/$SERVICE-chart/crds

# IRSA ì‚­ì œ
eksctl delete iamserviceaccount --cluster myeks --name ack-$SERVICE-controller --namespace ack-system

# namespace ì‚­ì œ >> ACK ëª¨ë“  ì‹¤ìŠµ í›„ ì‚­ì œ
kubectl delete namespace $ACK_K8S_NAMESPACE
```

**ACK EC2-Controller ì„¤ì¹˜ with Helm**

```
# ì„œë¹„ìŠ¤ëª… ë³€ìˆ˜ ì§€ì • ë° helm ì°¨íŠ¸ ë‹¤ìš´ë¡œë“œ
export SERVICE=ec2
export RELEASE_VERSION=$(curl -sL https://api.github.com/repos/aws-controllers-k8s/$SERVICE-controller/releases/latest | grep '"tag_name":' | cut -d'"' -f4 | cut -c 2-)
helm pull oci://public.ecr.aws/aws-controllers-k8s/$SERVICE-chart --version=$RELEASE_VERSION
tar xzvf $SERVICE-chart-$RELEASE_VERSION.tgz

# helm chart í™•ì¸
tree ~/$SERVICE-chart

# ACK EC2-Controller ì„¤ì¹˜
export ACK_SYSTEM_NAMESPACE=ack-system
export AWS_REGION=ap-northeast-2
helm install -n $ACK_SYSTEM_NAMESPACE ack-$SERVICE-controller --set aws.region="$AWS_REGION" ~/$SERVICE-chart

# ì„¤ì¹˜ í™•ì¸
helm list --namespace $ACK_SYSTEM_NAMESPACE
kubectl -n $ACK_SYSTEM_NAMESPACE get pods -l "app.kubernetes.io/instance=ack-$SERVICE-controller"
kubectl get crd | grep $SERVICE
dhcpoptions.ec2.services.k8s.aws             2023-06-11T10:59:14Z
elasticipaddresses.ec2.services.k8s.aws      2023-06-11T10:59:14Z
instances.ec2.services.k8s.aws               2023-06-11T10:59:14Z
internetgateways.ec2.services.k8s.aws        2023-06-11T10:59:14Z
natgateways.ec2.services.k8s.aws             2023-06-11T10:59:14Z
routetables.ec2.services.k8s.aws             2023-06-11T10:59:15Z
securitygroups.ec2.services.k8s.aws          2023-06-11T10:59:15Z
subnets.ec2.services.k8s.aws                 2023-06-11T10:59:15Z
transitgateways.ec2.services.k8s.aws         2023-06-11T10:59:15Z
vpcendpoints.ec2.services.k8s.aws            2023-06-11T10:59:15Z
vpcs.ec2.services.k8s.aws                    2023-06-11T10:59:15Z
```

**IRSA ì„¤ì • : ê¶Œì¥ ì •ì±… AmazonEC2FullAccess**

```
# Create an iamserviceaccount - AWS IAM role bound to a Kubernetes service account
eksctl create iamserviceaccount \
  --name ack-$SERVICE-controller \
  --namespace $ACK_SYSTEM_NAMESPACE \
  --cluster $CLUSTER_NAME \
  --attach-policy-arn $(aws iam list-policies --query 'Policies[?PolicyName==`AmazonEC2FullAccess`].Arn' --output text) \
  --override-existing-serviceaccounts --approve

# í™•ì¸ >> ì›¹ ê´€ë¦¬ ì½˜ì†”ì—ì„œ CloudFormation Stack >> IAM Role í™•ì¸
eksctl get iamserviceaccount --cluster $CLUSTER_NAME

# Inspecting the newly created Kubernetes Service Account, we can see the role we want it to assume in our pod.
kubectl get sa -n $ACK_SYSTEM_NAMESPACE
kubectl describe sa ack-$SERVICE-controller -n $ACK_SYSTEM_NAMESPACE

# Restart ACK service controller deployment using the following commands.
kubectl -n $ACK_SYSTEM_NAMESPACE rollout restart deploy ack-$SERVICE-controller-$SERVICE-chart

# IRSA ì ìš©ìœ¼ë¡œ Env, Volume ì¶”ê°€ í™•ì¸
kubectl describe pod -n $ACK_SYSTEM_NAMESPACE -l k8s-app=$SERVICE-chart
```

**VPC, Subnet ìƒì„± ë° ì‚­ì œ**

```
# [í„°ë¯¸ë„1] ëª¨ë‹ˆí„°ë§
while true; do aws ec2 describe-vpcs --query 'Vpcs[*].{VPCId:VpcId, CidrBlock:CidrBlock}' --output text; echo "-----"; sleep 1; done

# VPC ìƒì„±
cat <<EOF > vpc.yaml
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: VPC
metadata:
  name: vpc-tutorial-test
spec:
  cidrBlocks: 
  - 10.0.0.0/16
  enableDNSSupport: true
  enableDNSHostnames: true
EOF
 
kubectl apply -f vpc.yaml
vpc.ec2.services.k8s.aws/vpc-tutorial-test created

# VPC ìƒì„± í™•ì¸
kubectl get vpcs
kubectl describe vpcs
aws ec2 describe-vpcs --query 'Vpcs[*].{VPCId:VpcId, CidrBlock:CidrBlock}' --output text


# [í„°ë¯¸ë„1] ëª¨ë‹ˆí„°ë§
VPCID=$(kubectl get vpcs vpc-tutorial-test -o jsonpath={.status.vpcID})
while true; do aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPCID" --query 'Subnets[*].{SubnetId:SubnetId, CidrBlock:CidrBlock}' --output text; echo "-----"; sleep 1 ; done

# ì„œë¸Œë„· ìƒì„±
VPCID=$(kubectl get vpcs vpc-tutorial-test -o jsonpath={.status.vpcID})

cat <<EOF > subnet.yaml
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: Subnet
metadata:
  name: subnet-tutorial-test
spec:
  cidrBlock: 10.0.0.0/20
  vpcID: $VPCID
EOF
kubectl apply -f subnet.yaml
subnet.ec2.services.k8s.aws/subnet-tutorial-test created

# ì„œë¸Œë„· ìƒì„± í™•ì¸
kubectl get subnets
kubectl describe subnets
aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPCID" --query 'Subnets[*].{SubnetId:SubnetId, CidrBlock:CidrBlock}' --output text
```

**Create a VPC Workflow : VPC, Subnet, SG, RT, EIP, IGW, NATGW, Instance ìƒì„±**

![image](https://github.com/jiwonYun9332/AWES-1/blob/3ec2e102537ed760f90d4aa8d9ffd7d443ec5368/Study/images/124_image.jpg)

**vpc-workflow.yaml íŒŒì¼ ìƒì„±**

```
cat <<EOF > vpc-workflow.yaml
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: VPC
metadata:
  name: tutorial-vpc
spec:
  cidrBlocks: 
  - 10.0.0.0/16
  enableDNSSupport: true
  enableDNSHostnames: true
  tags:
    - key: name
      value: vpc-tutorial
---
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: InternetGateway
metadata:
  name: tutorial-igw
spec:
  vpcRef:
    from:
      name: tutorial-vpc
---
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: NATGateway
metadata:
  name: tutorial-natgateway1
spec:
  subnetRef:
    from:
      name: tutorial-public-subnet1
  allocationRef:
    from:
      name: tutorial-eip1
---
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: ElasticIPAddress
metadata:
  name: tutorial-eip1
spec:
  tags:
    - key: name
      value: eip-tutorial
---
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: RouteTable
metadata:
  name: tutorial-public-route-table
spec:
  vpcRef:
    from:
      name: tutorial-vpc
  routes:
  - destinationCIDRBlock: 0.0.0.0/0
    gatewayRef:
      from:
        name: tutorial-igw
---
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: RouteTable
metadata:
  name: tutorial-private-route-table-az1
spec:
  vpcRef:
    from:
      name: tutorial-vpc
  routes:
  - destinationCIDRBlock: 0.0.0.0/0
    natGatewayRef:
      from:
        name: tutorial-natgateway1
---
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: Subnet
metadata:
  name: tutorial-public-subnet1
spec:
  availabilityZone: ap-northeast-2a
  cidrBlock: 10.0.0.0/20
  mapPublicIPOnLaunch: true
  vpcRef:
    from:
      name: tutorial-vpc
  routeTableRefs:
  - from:
      name: tutorial-public-route-table
---
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: Subnet
metadata:
  name: tutorial-private-subnet1
spec:
  availabilityZone: ap-northeast-2a
  cidrBlock: 10.0.128.0/20
  vpcRef:
    from:
      name: tutorial-vpc
  routeTableRefs:
  - from:
      name: tutorial-private-route-table-az1
---
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: SecurityGroup
metadata:
  name: tutorial-security-group
spec:
  description: "ack security group"
  name: tutorial-sg
  vpcRef:
     from:
       name: tutorial-vpc
  ingressRules:
    - ipProtocol: tcp
      fromPort: 22
      toPort: 22
      ipRanges:
        - cidrIP: "0.0.0.0/0"
          description: "ingress"
EOF
```

```
# VPC í™˜ê²½ ìƒì„±
kubectl apply -f vpc-workflow.yaml

# [í„°ë¯¸ë„1] NATGW ìƒì„± ì™„ë£Œ í›„ tutorial-private-route-table-az1 ë¼ìš°íŒ… í…Œì´ë¸” IDê°€ í™•ì¸ë˜ê³  ê·¸í›„ tutorial-private-subnet1 ì„œë¸Œë„·IDê°€ í™•ì¸ë¨ > 5ë¶„ ì •ë„ ì‹œê°„ ì†Œìš”
watch -d kubectl get routetables,subnet

# VPC í™˜ê²½ ìƒì„± í™•ì¸
kubectl describe vpcs
kubectl describe internetgateways
kubectl describe routetables
kubectl describe natgateways
kubectl describe elasticipaddresses
kubectl describe securitygroups

# ë°°í¬ ë„ì¤‘ 2ê°œì˜ ì„œë¸Œë„· ìƒíƒœ ì •ë³´ ë¹„êµ í•´ë³´ì
kubectl describe subnets
Name:         tutorial-private-subnet1
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  ec2.services.k8s.aws/v1alpha1
Kind:         Subnet
Metadata:
  Creation Timestamp:  2023-06-11T11:07:42Z
  Generation:          1
  Resource Version:    51760
  UID:                 e66a692e-9466-43e8-81de-fd9c7581e543
Spec:
  Availability Zone:  ap-northeast-2a
  Cidr Block:         10.0.128.0/20
  Route Table Refs:
    From:
      Name:  tutorial-private-route-table-az1
  Vpc Ref:
    From:
      Name:  tutorial-vpc
Status:
  Conditions:
    Last Transition Time:  2023-06-11T11:08:04Z
    Message:               Reference resolution failed
    Reason:                the referenced resource is not synced yet. resource:RouteTable, namespace:default, name:tutorial-private-route-table-az1
    Status:                Unknown
    Type:                  ACK.ReferencesResolved
Events:                    <none>


Name:         tutorial-public-subnet1
Namespace:    default
Labels:       <none>
Annotations:  <none>
API Version:  ec2.services.k8s.aws/v1alpha1
Kind:         Subnet
Metadata:
  Creation Timestamp:  2023-06-11T11:07:42Z
  Finalizers:
    finalizers.ec2.services.k8s.aws/Subnet
  Generation:        2
  Resource Version:  51667
  UID:               3a83bc6c-ae75-48a7-b790-3346fe1dbb60
Spec:
  assignIPv6AddressOnCreation:  false
  Availability Zone:            ap-northeast-2a
  Availability Zone ID:         apne2-az1
  Cidr Block:                   10.0.0.0/20
  enableDNS64:                  false
  ipv6Native:                   false
  Map Public IP On Launch:      true
  Route Table Refs:
    From:
      Name:  tutorial-public-route-table
  Vpc Ref:
    From:
      Name:  tutorial-vpc
Status:
  Ack Resource Metadata:
    Arn:                       arn:aws:ec2:ap-northeast-2:485702506058:subnet/subnet-05233ba5b9ff93158
    Owner Account ID:          485702506058
    Region:                    ap-northeast-2
  Available IP Address Count:  4091
  Conditions:
    Last Transition Time:           2023-06-11T11:07:45Z
    Status:                         True
    Type:                           ACK.ReferencesResolved
    Last Transition Time:           2023-06-11T11:07:45Z
    Message:                        Resource synced successfully
    Reason:
    Status:                         True
    Type:                           ACK.ResourceSynced
  Default For AZ:                   false
  Map Customer Owned IP On Launch:  false
  Owner ID:                         485702506058
  Private DNS Name Options On Launch:
  State:      available
  Subnet ID:  subnet-05233ba5b9ff93158
Events:       <none>
```

![image](https://github.com/jiwonYun9332/AWES-1/blob/5c08c699a57892b18d73b147b3fdb6d26dc90dbc/Study/images/125_image.jpg)

**í¼ë¸”ë¦­ ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ ìƒì„±**

```
# public ì„œë¸Œë„· ID í™•ì¸
PUBSUB1=$(kubectl get subnets tutorial-public-subnet1 -o jsonpath={.status.subnetID})
echo $PUBSUB1

# ë³´ì•ˆê·¸ë£¹ ID í™•ì¸
TSG=$(kubectl get securitygroups tutorial-security-group -o jsonpath={.status.id})
echo $TSG

# Amazon Linux 2 ìµœì‹  AMI ID í™•ì¸
AL2AMI=$(aws ec2 describe-images --owners amazon --filters "Name=name,Values=amzn2-ami-hvm-2.0.*-x86_64-gp2" --query 'Images[0].ImageId' --output text)
echo $AL2AMI

# ê°ì ìì‹ ì˜ SSH í‚¤í˜ì–´ ì´ë¦„ ë³€ìˆ˜ ì§€ì •
MYKEYPAIR=<ê°ì ìì‹ ì˜ SSH í‚¤í˜ì–´ ì´ë¦„>
MYKEYPAIR=kp-gasida

# ë³€ìˆ˜ í™•ì¸ > íŠ¹íˆ ì„œë¸Œë„· IDê°€ í™•ì¸ë˜ì—ˆëŠ”ì§€ ê¼­ í™•ì¸í•˜ì!
echo $PUBSUB1 , $TSG , $AL2AMI , $MYKEYPAIR


# [í„°ë¯¸ë„1] ëª¨ë‹ˆí„°ë§
while true; do aws ec2 describe-instances --query "Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}" --filters Name=instance-state-name,Values=running --output table; date ; sleep 1 ; done

# public ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
cat <<EOF > tutorial-bastion-host.yaml
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: Instance
metadata:
  name: tutorial-bastion-host
spec:
  imageID: $AL2AMI # AL2 AMI ID - ap-northeast-2
  instanceType: t3.medium
  subnetID: $PUBSUB1
  securityGroupIDs:
  - $TSG
  keyName: $MYKEYPAIR
  tags:
    - key: producer
      value: ack
EOF
kubectl apply -f tutorial-bastion-host.yaml

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í™•ì¸
kubectl get instance
kubectl describe instance
aws ec2 describe-instances --query "Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}" --filters Name=instance-state-name,Values=running --output table
------------------------------------------------------------------
|                        DescribeInstances                       |
+----------------+-----------------+------------------+----------+
|  InstanceName  |  PrivateIPAdd   |   PublicIPAdd    | Status   |
+----------------+-----------------+------------------+----------+
|  myeks-ng1-Node|  192.168.3.61   |  13.209.15.136   |  running |
|  myeks-ng1-Node|  192.168.2.130  |  15.164.174.34   |  running |
|  myeks-bastion |  192.168.1.100  |  3.36.69.66      |  running |
|  myeks-ng1-Node|  192.168.1.97   |  54.180.137.188  |  running |
|  None          |  10.0.12.205    |  43.202.49.136   |  running |
+----------------+-----------------+------------------+----------+
```

**public ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ ì ‘ì†**

```
ssh -i <ìì‹ ì˜ í‚¤í˜ì–´íŒŒì¼> ec2-user@<public ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ í¼ë¸”ë¦­IP>
------
# public ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ ì ‘ì† í›„ ì™¸ë¶€ ì¸í„°ë„· í†µì‹  ì—¬ë¶€ í™•ì¸ 
ping -c 2 8.8.8.8
```

**ë³´ì•ˆ ê·¸ë£¹ ì •ì±… ìˆ˜ì • : egress ê·œì¹™ ì¶”ê°€**

```
cat <<EOF > modify-sg.yaml
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: SecurityGroup
metadata:
  name: tutorial-security-group
spec:
  description: "ack security group"
  name: tutorial-sg
  vpcRef:
     from:
       name: tutorial-vpc
  ingressRules:
    - ipProtocol: tcp
      fromPort: 22
      toPort: 22
      ipRanges:
        - cidrIP: "0.0.0.0/0"
          description: "ingress"
  egressRules:
    - ipProtocol: '-1'
      ipRanges:
        - cidrIP: "0.0.0.0/0"
          description: "egress"
EOF
kubectl apply -f modify-sg.yaml

# ë³€ê²½ í™•ì¸ >> ë³´ì•ˆê·¸ë£¹ì— ì•„ì›ƒë°”ìš´ë“œ ê·œì¹™ í™•ì¸
kubectl logs -n $ACK_SYSTEM_NAMESPACE -l k8s-app=ec2-chart -f
```

**public ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ ì ‘ì† í›„ ì™¸ë¶€ ì¸í„°ë„· í†µì‹  í™•ì¸**

```
ssh -i <ìì‹ ì˜ í‚¤í˜ì–´íŒŒì¼> ec2-user@<public ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ í¼ë¸”ë¦­IP>
------
# public ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ ì ‘ì† í›„ ì™¸ë¶€ ì¸í„°ë„· í†µì‹  ì—¬ë¶€ í™•ì¸ 
ping -c 10 8.8.8.8
curl ipinfo.io/ip ; echo # ì¶œë ¥ë˜ëŠ” ê³µì¸IPëŠ” ë¬´ì—‡ì¸ê°€?
exit
------
```

**í”„ë¼ì´ë¹— ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ ìƒì„±**

```
# private ì„œë¸Œë„· ID í™•ì¸ >> NATGW ìƒì„± ì™„ë£Œ í›„ RT/SubnetIDê°€ í™•ì¸ë˜ì–´ ë‹¤ì†Œ ì‹œê°„ í•„ìš”í•¨
PRISUB1=$(kubectl get subnets tutorial-private-subnet1 -o jsonpath={.status.subnetID})
echo $PRISUB1

# ë³€ìˆ˜ í™•ì¸ > íŠ¹íˆ private ì„œë¸Œë„· IDê°€ í™•ì¸ë˜ì—ˆëŠ”ì§€ ê¼­ í™•ì¸í•˜ì!
echo $PRISUB1 , $TSG , $AL2AMI , $MYKEYPAIR

# private ì„œë¸Œë„·ì— ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
cat <<EOF > tutorial-instance-private.yaml
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: Instance
metadata:
  name: tutorial-instance-private
spec:
  imageID: $AL2AMI # AL2 AMI ID - ap-northeast-2
  instanceType: t3.medium
  subnetID: $PRISUB1
  securityGroupIDs:
  - $TSG
  keyName: $MYKEYPAIR
  tags:
    - key: producer
      value: ack
EOF
kubectl apply -f tutorial-instance-private.yaml

# ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í™•ì¸
kubectl get instance
kubectl describe instance
aws ec2 describe-instances --query "Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}" --filters Name=instance-state-name,Values=running --output table
```

**ì‹¤ìŠµ í›„ ë¦¬ì†ŒìŠ¤ ì‚­ì œ**

```
kubectl delete -f tutorial-bastion-host.yaml && kubectl delete -f tutorial-instance-private.yaml
kubectl delete -f vpc-workflow.yaml  # vpc ê´€ë ¨ ëª¨ë“  ë¦¬ì†ŒìŠ¤ë“¤ ì‚­ì œì—ëŠ” ë‹¤ì†Œ ì‹œê°„ì´ ì†Œìš”ë¨
```

### 2. Flux

**fluxë€?**

![image](https://github.com/jiwonYun9332/AWES-1/blob/cc7a4c70cddfbf8ccb26bc7d3d7975c53f876fbe/Study/images/120_image.jpg)

fluxëŠ” ì¿ ë²„ë„¤í‹°ìŠ¤ë¥¼ ìœ„í•œ gitops ë„êµ¬ì´ë‹¤. fluxëŠ” gitì— ìˆëŠ” ì¿ ë²„ë„¤í‹°ìŠ¤ë¥¼ manifestë¥¼ ì½ê³ , ì¿ ë²„ë„¤í‹°ìŠ¤ì— manifestë¥¼ ë°°í¬í•œë‹¤.

**fluxì™€ argocd ë¹„êµ**

fluxëŠ” argocdì™€ ê°™ì€ ì—­í• ì„ í•˜ê³  ìˆì–´ ë¹„êµëŒ€ìƒìœ¼ë¡œ ìì£¼ ì–¸ê¸‰ëœë‹¤.

argocdëŠ” kustomizeë¥¼ ì‚¬ìš©í•  ë•Œ ë””í´íŠ¸ ì„¤ì •ì´ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì—, argocd configmapì—ì„œ kustomizeì˜µì…˜ì„ í•œë•€í•œë•€ ì„¤ì •í•´ì•¼ í•œë‹¤.
ë°˜ë©´ì—  fluxëŠ” ë°”ë¡œ kustomizeë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ì´ ë˜ì–´ ìˆë‹¤.

fluxëŠ” ì¿ ë²„ë„¤í‹°ìŠ¤ operatoríŒ¨í„´ì„ ì‚¬ìš©í•œë‹¤. ì‚¬ìš©ìê°€ ì¿ ë²„ë„¤í‹°ìŠ¤ì— ë°°í¬í•  ë‚´ìš©ì„ flux CRDë¡œ ì •ì˜í•˜ë©´, flux controllerê°€ CRDë¥¼ ì½ì–´ ë¦¬ì†ŒìŠ¤ë¥¼ ì¿ ë²„ë„¤í‹°ìŠ¤ì— ë°°í¬í•œë‹¤.

![image](https://github.com/jiwonYun9332/AWES-1/blob/cc7a4c70cddfbf8ccb26bc7d3d7975c53f876fbe/Study/images/121_image.jpg)

í•µì‹¬ CRDëŠ” ì†ŒìŠ¤(source)ì™€ ì• í”Œë¦¬ì¼€ì´ì…˜(application)ì…ë‹ˆë‹¤. ì†ŒìŠ¤ëŠ” git, helm registry ë“± manifest ì£¼ì†Œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. 
ì• í”Œë¦¬ì¼€ì´ì…˜ì€ helm ë˜ëŠ” kustomizeë¥¼ ì‚¬ìš©í•˜ì—¬ manifestë¥¼ ì¿ ë²„ë„¤í‹°ìŠ¤ì— ë°°í¬í•œë‹¤.

**Flux CLI ì„¤ì¹˜ ë° Bootstrap**

Github Token ë°œê¸‰

![image](https://github.com/jiwonYun9332/AWES-1/blob/6870a6ce454de27974b6bbbcbcb269a923c4e75a/Study/images/126_image.jpg)

```
# Flux CLI ì„¤ì¹˜
curl -s https://fluxcd.io/install.sh | sudo bash
. <(flux completion bash)

# ë²„ì „ í™•ì¸
flux --version
flux version 2.0.0-rc.5

# ìì‹ ì˜ Github í† í°ê³¼ ìœ ì €ì´ë¦„ ë³€ìˆ˜ ì§€ì •
export GITHUB_TOKEN=<your-token>
export GITHUB_USER=<your-username>
# Bootstrap
## Creates a git repository fleet-infra on your GitHub account.
## Adds Flux component manifests to the repository.
## Deploys Flux Components to your Kubernetes Cluster.
## Configures Flux components to track the path /clusters/my-cluster/ in the repository.
flux bootstrap github \
  --owner=$GITHUB_USER \
  --repository=fleet-infra \
  --branch=main \
  --path=./clusters/my-cluster \
  --personal

# ì„¤ì¹˜ í™•ì¸
kubectl get pods -n flux-system
kubectl get-all -n flux-system
kubectl get crd | grep fluxc
kubectl get gitrepository -n flux-system
flux-system   ssh://git@github.com/jiwonYun9332/fleet-infra   22s   True    stored artifact for revision 'main@sha1:111c32fda17cfee0f1e10929e350cdae25773b0a'
```

**gitops ë„êµ¬ ì„¤ì¹˜ - ë§í¬ â†’ flux ëŒ€ì‹œë³´ë“œ ì„¤ì¹˜ : admin / password**

```
# gitops ë„êµ¬ ì„¤ì¹˜
curl --silent --location "https://github.com/weaveworks/weave-gitops/releases/download/v0.24.0/gitops-$(uname)-$(uname -m).tar.gz" | tar xz -C /tmp
sudo mv /tmp/gitops /usr/local/bin
gitops version

# flux ëŒ€ì‹œë³´ë“œ ì„¤ì¹˜
PASSWORD="password"
gitops create dashboard ww-gitops --password=$PASSWORD

# í™•ì¸
flux -n flux-system get helmrelease
kubectl -n flux-system get pod,svc
```

**Ingress ì„¤ì •**

```
CERT_ARN=`aws acm list-certificates --query 'CertificateSummaryList[].CertificateArn[]' --output text`
echo $CERT_ARN

# Ingress ì„¤ì •
cat <<EOT > gitops-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: gitops-ingress
  annotations:
    alb.ingress.kubernetes.io/certificate-arn: $CERT_ARN
    alb.ingress.kubernetes.io/group.name: study
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}, {"HTTP":80}]'
    alb.ingress.kubernetes.io/load-balancer-name: myeks-ingress-alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/ssl-redirect: "443"
    alb.ingress.kubernetes.io/success-codes: 200-399
    alb.ingress.kubernetes.io/target-type: ip
spec:
  ingressClassName: alb
  rules:
  - host: gitops.$MyDomain
    http:
      paths:
      - backend:
          service:
            name: ww-gitops-weave-gitops
            port:
              number: 9001
        path: /
        pathType: Prefix
EOT
kubectl apply -f gitops-ingress.yaml -n flux-system

# ë°°í¬ í™•ì¸
kubectl get ingress -n flux-system

# GitOps ì ‘ì† ì •ë³´ í™•ì¸ >> ì›¹ ì ‘ì† í›„ ì •ë³´ í™•ì¸
echo -e "GitOps Web https://gitops.$MyDomain"
```

**ì‚­ì œ**

```
flux uninstall --namespace=flux-system
```

3. ArgoCD

**GitOpsë€?**

GitOpsëŠ” DevOpsì˜ ì‹¤ì²œ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ,
ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë°°í¬ì™€ ìš´ì˜ì— ê´€ë ¨ëœ ëª¨ë“  ìš”ì†Œë“¤ì„ Gitì—ì„œ ê´€ë¦¬í•œë‹¤ëŠ” ëœ»ì´ë‹¤.

GitOpsëŠ” Git Pull ìš”ì²­ì„ ì‚¬ìš©í•˜ì—¬ ì¸í”„ë¼ í”„ë¡œë¹„ì €ë‹ ë° ë°°í¬ë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•œë‹¤.

Git ë ˆí¬ì§€í† ë¦¬ì—ëŠ” ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœê°€ í¬í•¨ë˜ì–´ ìˆì–´ ì‹œìŠ¤í…œ ìƒíƒœì˜ ë³€í™” ì¶”ì´ë¥¼ í™•ì¸, ê°ì‚¬í•  ìˆ˜ ìˆë‹¤.

![image](https://github.com/jiwonYun9332/AWES-1/blob/385ae106cde740cb70bdec952d2fea69dca5a043/Study/images/122_image.jpg)

GitOpsì˜ ì›ì¹™

- ëª¨ë“  ì‹œìŠ¤í…œì€ ì„ ì–¸ì ìœ¼ë¡œ ì„ ì–¸ë˜ì–´ì•¼ í•œë‹¤.
- ì‹œìŠ¤í…œì˜ ìƒíƒœëŠ” Gitì˜ ë²„ì „ì„ ë”°ë¼ê°„ë‹¤.
- ìŠ¹ì¸ëœ ë³€í™”ëŠ” ìë™ìœ¼ë¡œ ì‹œìŠ¤í…œì— ëŒ€ì‘í•œë‹¤.
- ë°°í¬ì— ì‹¤íŒ¨í•˜ë©´ ì´ë¥¼ ì‚¬ìš©ìì—ê²Œ ê²½ê³ í•´ì•¼ í•œë‹¤.

ArgoCDë€?

ArgoCDëŠ” GitOps ë°©ì‹ìœ¼ë¡œ ê´€ë¦¬ë˜ëŠ” Manifest(yaml) íŒŒì¼ì˜ ë³€ê²½ì‚¬í•­ì„ ê°ì‹œí•˜ë©°, í˜„ì¬ ë°°í¬ëœ í™˜ê²½ì˜ ìƒíƒœì™€ Github Manifest íŒŒì¼ì— ì •ì˜ëœ ìƒíƒœë¥¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.

![image](https://github.com/jiwonYun9332/AWES-1/blob/385ae106cde740cb70bdec952d2fea69dca5a043/Study/images/123_image.jpg)

```
"ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes."
```

ArgoCD ì„¤ì¹˜

```
#Install Argo CD
kubectl create namespace argocd

# ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ì ìš©í•˜ì—¬ í•„ìš”í•œ Argo CD ì¿ ë²„ë„¤í‹°ìŠ¤ ì˜¤ë¸Œì íŠ¸ë¥¼ ì„¤ì¹˜
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Download Argo CD CLI
sudo curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
sudo chmod +x /usr/local/bin/argocd
argocd: v2.7.4+a33baa3
  BuildDate: 2023-06-05T19:16:50Z
  GitCommit: a33baa301fe61b899dc8bbad9e554efbc77e0991
  GitTreeState: clean
  GoVersion: go1.19.9
  Compiler: gc
  Platform: linux/amd64
FATA[0000] Argo CD server address unspecified
```

ArgoCDëŠ” ì¿ ë²„ë„¤í‹°ìŠ¤ë¥¼ ìœ„í•œ CD(Continuous Delivery) íˆ´ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.

ì¿ ë²„ë„¤í‹°ìŠ¤ì˜ êµ¬ì„± ìš”ì†Œë“¤ì„ ê´€ë¦¬ ë° ë°°í¬í•˜ê¸° ìœ„í•´ì„œëŠ” Manifest íŒŒì¼ì„ êµ¬ì„±í•˜ì—¬ ì‹¤í–‰í•´ì•¼ í•˜ë©°,
ì´ëŸ¬í•œ íŒŒì¼ë“¤ì€ ê³„ì†í•´ì„œ ë³€ê²½ë˜ê¸° ë•Œë¬¸ì— ì§€ì†ì ì¸ ê´€ë¦¬ê°€ í•„ìš”í•˜ë‹¤.
ì´ë¥¼ ê°„í¸í•˜ê²Œ Gitìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ì‹ì´ GitOpsì´ë‹¤, GitOpsë¥¼ ì‹¤í˜„ì‹œí‚¤ë©° ì¿ ë²„ë„¤í‹°ìŠ¤ì— ë°°í¬ê¹Œì§€ í•´ì£¼ëŠ” íˆ´ì´ ArgoCDì´ë‹¤.

**Access Argo CD API Server**

```
# Expose via public LB
export ARGOCD_SERVER=`kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'`
echo $ARGOCD_SERVER

# Login (2 Options)
argocd login $ARGOCD_SERVER --username admin --password $ARGO_PWD --insecure
```

![image](https://github.com/jiwonYun9332/AWES-1/blob/7a5854c1d45392dd366bbfd2eecd768a32a0f5a7/Study/images/127_image.jpg)

**ê°„ë‹¨ ì •ë¦¬**

í˜•ìƒ ê´€ë¦¬ ë„êµ¬ì¸ 'Git' ì„ í†µí•´ ê°œë°œìì—ê²Œ ìµìˆ™í•œ ë°©ì‹ìœ¼ë¡œ ì¸í”„ë¼ ë˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„ ì–¸ì ì¸ ì„¤ì •íŒŒì¼ì„ ê´€ë¦¬í•˜ê³  ë°°í¬í•˜ëŠ” ì¼ë ¨ì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ë§í•œë‹¤

ArgoCDë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ GitOps í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ K8S í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ë¡œ ì–´í”Œë¦¬ì¼€ì´ì…˜(ë„ì»¤ ì´ë¯¸ì§€)ì´ ë°°í¬ë˜ëŠ” ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ë‹¤.

![image](https://github.com/jiwonYun9332/AWES-1/blob/116c4d23a6f0c6008705f4f6bb293688cffca1bd/Study/images/128_image.jpg)

ìœ„ ë„ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë°°í¬ í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- íŠ¹ì • ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì˜ ë³€ê²½ì‚¬í•­ì´ Source repoì— commit ëœë‹¤.
- Jenkinsì˜ Build Jobì´ í•´ë‹¹ ë³€ê²½ì‚¬í•­ì„ ê°ì§€í•˜ê³  Docker Image ë¹Œë“œë¥¼ ìˆ˜í–‰í•œë‹¤.
- ë¹Œë“œì˜ ê²°ê³¼ë¬¼ì€ Amazon ECR Repositoryì— Pushëœë‹¤. (ì´ë¯¸ì§€ TagëŠ” Jenkinsì˜ BuildNumber)
- í•´ë‹¹ Tagë¥¼ kustomize ëª…ë ¹ì–´ë¡œ GitOps repoì˜ deployement manifestì— ì—…ë°ì´íŠ¸í•œë‹¤.
- 4ë‹¨ê³„ì—ì„œ ì—…ë°ì´íŠ¸í•œ deploymentì˜ ë³€ê²½ì‚¬í•­ì´ GitOps repoì— commit ëœë‹¤.
- GitOpsë¥¼ í–¥í•´ pollingì„ ìˆ˜í–‰í•˜ê³  ìˆë˜ ArgoCDê°€ ë³€ê²½ì‚¬í•­ì„ ê°ì§€í•˜ê³  Synchronize(ë°°í¬)ë¥¼ ìˆ˜í–‰í•œë‹¤.

4. Crossplane

![image](https://github.com/jiwonYun9332/AWES-1/blob/e88f2bd6b95b4031b3d061680e28daf990dd1352/Study/images/129_image.jpg)

Crossplaneë€?

Crossplaneì€ 2018ì€ upbound íšŒì‚¬ì—ì„œ ë§Œë“  í”„ë¡œì íŠ¸ì´ë©° 2020ë…„ 7ì›” CNCFì˜ Sandbox í”„ë¡œì íŠ¸ê°€ ë˜ì—ˆë‹¤.

Crossplaneì€ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ í•„ìš”í•œ ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜ë¥¼ Kubernetesì—ì„œ ì§ì ‘ ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í”„ë¡œì íŠ¸ì´ë‹¤.

**Crossplane ì„¤ì¹˜**

```
kubectl create namespace crossplane-system
namespace/crossplane-system created

helm repo add crossplane-stable https://charts.crossplane.io/stable
"crossplane-stable" has been added to your repositories

helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "eks" chart repository
...Successfully got an update from the "argo" chart repository
...Successfully got an update from the "crossplane-stable" chart repository
...Successfully got an update from the "prometheus-community" chart repository
Update Complete. âˆHappy Helming!âˆ

helm install crossplane --namespace crossplane-system crossplane-stable/crossplane --version 1.3.1
NAME: crossplane
LAST DEPLOYED: Sun Jun 11 20:36:57 2023
NAMESPACE: crossplane-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Release: crossplane

Chart Name: crossplane
Chart Description: Crossplane is an open source Kubernetes add-on that enables platform teams to assemble infrastructure from multiple vendors, and expose higher level self-service APIs for application teams to consume.
Chart Version: 1.3.1
Chart Application Version: 1.3.1

Kube Version: v1.24.14-eks-c12679a

# ì„¤ì¹˜ í™•ì¸
helm list -n crossplane-system
NAME            NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
crossplane      crossplane-system       1               2023-06-11 20:36:57.428323574 +0900 KST deployed        crossplane-1.3.1        1.3.1

# ì„¤ì¹˜ ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl -n crossplane-system get all
NAME                                           READY   STATUS    RESTARTS   AGE
pod/crossplane-5b4d54cbf9-5lhp7                1/1     Running   0          31s
pod/crossplane-rbac-manager-6d7f75b447-hzzlh   1/1     Running   0          31s

NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/crossplane                1/1     1            1           31s
deployment.apps/crossplane-rbac-manager   1/1     1            1           31s

NAME                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/crossplane-5b4d54cbf9                1         1         1       31s
replicaset.apps/crossplane-rbac-manager-6d7f75b447   1         1         1       31s

# crossplane CLI ì„¤ì¹˜
curl -sL https://raw.githubusercontent.com/crossplane/crossplane/master/install.sh | sh
kubectl plugin downloaded successfully! Run the following commands to finish installing it:

sudo mv kubectl-crossplane /usr/local/bin
kubectl crossplane --help

Visit https://crossplane.io to get started. ğŸš€
Have a nice day! ğŸ‘‹\n

# ì„¤ì¹˜ í™•ì¸
kubectl crossplane --help
```

**(ì‹¤ìŠµ ì™„ë£Œ í›„) ìì›  ì‚­ì œ**
```
# Flux ì‹¤ìŠµ ë¦¬ì†ŒìŠ¤ ì‚­ì œ
flux uninstall --namespace=flux-system

# Helm Chart ì‚­ì œ
helm uninstall -n monitoring kube-prometheus-stack

# EKS í´ëŸ¬ìŠ¤í„° ì‚­ì œ
eksctl delete cluster --name $CLUSTER_NAME && aws cloudformation delete-stack --stack-name $CLUSTER_NAME
```



